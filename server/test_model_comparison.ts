/**
 * Grok vs GLM æ¨¡å‹å¯¹æ¯”æµ‹è¯•
 *
 * æµ‹è¯•ç»´åº¦ï¼š
 * 1. Tool Call æˆåŠŸç‡
 * 2. å“åº”å»¶è¿Ÿ
 * 3. è¾“å‡ºç¨³å®šæ€§
 * 4. å†…å®¹è´¨é‡
 *
 * ç”¨æ³•: npx tsx server/test_model_comparison.ts
 */

import { ENV } from "./_core/env";
import { BaseAgent } from "./_core/agent/base-agent";
import { executeStockTool, stockTools } from "./_core/stockTools";
import type { ToolDefinition, AgentConfig } from "./_core/agent/types";

// ==================== æ¨¡å‹é…ç½® ====================

const MODELS = {
  grok: {
    name: "Grok",
    apiUrl: ENV.grokApiUrl,
    apiKey: ENV.grokApiKey,
    model: ENV.grokModel || "grok-3-mini",
  },
  glm: {
    name: "GLM",
    apiUrl: ENV.glmApiUrl,
    apiKey: ENV.glmApiKey,
    model: ENV.glmModel || "glm-4-plus",
  },
};

// ==================== æµ‹è¯•ç”¨ Agent ====================

class TestAgent extends BaseAgent {
  private modelConfig: typeof MODELS.grok;

  constructor(modelType: "grok" | "glm") {
    const modelConfig = MODELS[modelType];

    const testTools: ToolDefinition[] = stockTools.filter(t =>
      [
        "get_stock_quote",
        "analyze_stock_technical",
        "get_fund_flow",
        "get_current_datetime",
      ].includes(t.function.name)
    ) as ToolDefinition[];

    super({
      name: `TestAgent-${modelConfig.name}`,
      description: `Test agent using ${modelConfig.name}`,
      systemPrompt: `ä½ æ˜¯ä¸€ä¸ªè‚¡ç¥¨åˆ†æåŠ©æ‰‹ã€‚åˆ†æè‚¡ç¥¨æ—¶å¿…é¡»è°ƒç”¨å·¥å…·è·å–æ•°æ®ï¼Œä¸è¦ç¼–é€ æ•°æ®ã€‚`,
      tools: testTools,
      maxIterations: 5,
      temperature: 0.5,
      model: modelConfig.model,
      verbose: false,
    });

    this.modelConfig = modelConfig;
    this.registerTestTools();
  }

  private registerTestTools(): void {
    const toolNames = [
      "get_stock_quote",
      "analyze_stock_technical",
      "get_fund_flow",
      "get_current_datetime",
    ];
    for (const name of toolNames) {
      this.registerTool(name, async args => executeStockTool(name, args));
    }
  }

  protected override async callLLM() {
    const payload: any = {
      model: this.modelConfig.model,
      messages: this.state.messages,
      max_tokens: 4096,
      temperature: 0.5,
    };

    if (this.config.tools.length > 0) {
      payload.tools = this.config.tools;
      payload.tool_choice = "auto";
    }

    const response = await fetch(
      `${this.modelConfig.apiUrl}/chat/completions`,
      {
        method: "POST",
        headers: {
          "Content-Type": "application/json",
          Authorization: `Bearer ${this.modelConfig.apiKey}`,
        },
        body: JSON.stringify(payload),
      }
    );

    if (!response.ok) {
      const error = await response.text();
      throw new Error(
        `${this.modelConfig.name} Error: ${response.status} - ${error.slice(0, 200)}`
      );
    }

    const data = await response.json();
    const message = data.choices?.[0]?.message;

    return {
      content: message?.content || "",
      tool_calls: message?.tool_calls,
      usage: data.usage,
    };
  }
}

// ==================== æµ‹è¯•ç”¨ä¾‹ ====================

interface TestResult {
  model: string;
  testCase: string;
  success: boolean;
  toolCallCount: number;
  toolsUsed: string[];
  latency: number;
  outputLength: number;
  error?: string;
  output?: string;
}

const TEST_CASES = [
  {
    name: "ç®€å•æŸ¥è¯¢",
    prompt: "å¸®æˆ‘æŸ¥ä¸€ä¸‹æ¯”äºšè¿ªçš„è‚¡ä»·",
    expectedTools: ["get_stock_quote"],
  },
  {
    name: "æŠ€æœ¯åˆ†æ",
    prompt: "åˆ†æä¸€ä¸‹ 002594 çš„æŠ€æœ¯é¢",
    expectedTools: ["analyze_stock_technical"],
  },
  {
    name: "å¤šå·¥å…·è°ƒç”¨",
    prompt: "å…¨é¢åˆ†ææ¯”äºšè¿ªï¼ŒåŒ…æ‹¬è¡Œæƒ…ã€æŠ€æœ¯é¢å’Œèµ„é‡‘æµå‘",
    expectedTools: [
      "get_stock_quote",
      "analyze_stock_technical",
      "get_fund_flow",
    ],
  },
  {
    name: "æ—¶é—´æ„ŸçŸ¥",
    prompt: "ä»Šå¤©æ˜¯å‡ å·ï¼Ÿç„¶åæŸ¥ä¸€ä¸‹èŒ…å°çš„è‚¡ä»·",
    expectedTools: ["get_current_datetime", "get_stock_quote"],
  },
];

// ==================== è¿è¡Œæµ‹è¯• ====================

async function runSingleTest(
  modelType: "grok" | "glm",
  testCase: (typeof TEST_CASES)[0]
): Promise<TestResult> {
  const startTime = Date.now();
  const agent = new TestAgent(modelType);

  try {
    const response = await agent.run(testCase.prompt);
    const stats = agent.getToolStats();

    return {
      model: MODELS[modelType].name,
      testCase: testCase.name,
      success: true,
      toolCallCount: stats.reduce((acc, s) => acc + s.count, 0),
      toolsUsed: stats.map(s => s.name),
      latency: Date.now() - startTime,
      outputLength: response.length,
      output: response.slice(0, 300),
    };
  } catch (error: any) {
    return {
      model: MODELS[modelType].name,
      testCase: testCase.name,
      success: false,
      toolCallCount: 0,
      toolsUsed: [],
      latency: Date.now() - startTime,
      outputLength: 0,
      error: error.message,
    };
  }
}

async function runComparisonTest() {
  console.log("ğŸ”¬ Grok vs GLM æ¨¡å‹å¯¹æ¯”æµ‹è¯•\n");
  console.log("=".repeat(70));

  // æ£€æŸ¥ API Key
  if (!ENV.grokApiKey) {
    console.error("âŒ GROK_API_KEY æœªé…ç½®");
    return;
  }
  if (!ENV.glmApiKey) {
    console.error("âŒ GLM_API_KEY æœªé…ç½®");
    return;
  }

  console.log(`ğŸ“Œ Grok æ¨¡å‹: ${MODELS.grok.model}`);
  console.log(`ğŸ“Œ GLM æ¨¡å‹: ${MODELS.glm.model}\n`);

  const results: TestResult[] = [];

  for (const testCase of TEST_CASES) {
    console.log(`\nğŸ“ æµ‹è¯•: ${testCase.name}`);
    console.log(`   é—®é¢˜: "${testCase.prompt}"`);
    console.log("-".repeat(70));

    // æµ‹è¯• Grok
    console.log("   ğŸŸ¢ è¿è¡Œ Grok...");
    const grokResult = await runSingleTest("grok", testCase);
    results.push(grokResult);

    if (grokResult.success) {
      console.log(
        `      âœ… æˆåŠŸ | å»¶è¿Ÿ: ${grokResult.latency}ms | å·¥å…·: ${grokResult.toolsUsed.join(", ") || "æ— "}`
      );
      console.log(`      ğŸ“„ è¾“å‡º: ${grokResult.output?.slice(0, 100)}...`);
    } else {
      console.log(`      âŒ å¤±è´¥: ${grokResult.error}`);
    }

    // æµ‹è¯• GLM
    console.log("   ğŸ”µ è¿è¡Œ GLM...");
    const glmResult = await runSingleTest("glm", testCase);
    results.push(glmResult);

    if (glmResult.success) {
      console.log(
        `      âœ… æˆåŠŸ | å»¶è¿Ÿ: ${glmResult.latency}ms | å·¥å…·: ${glmResult.toolsUsed.join(", ") || "æ— "}`
      );
      console.log(`      ğŸ“„ è¾“å‡º: ${glmResult.output?.slice(0, 100)}...`);
    } else {
      console.log(`      âŒ å¤±è´¥: ${glmResult.error}`);
    }
  }

  // æ±‡æ€»ç»Ÿè®¡
  console.log("\n" + "=".repeat(70));
  console.log("ğŸ“Š æ±‡æ€»ç»Ÿè®¡\n");

  const grokResults = results.filter(r => r.model === "Grok");
  const glmResults = results.filter(r => r.model === "GLM");

  const summarize = (modelResults: TestResult[], name: string) => {
    const successCount = modelResults.filter(r => r.success).length;
    const avgLatency =
      modelResults
        .filter(r => r.success)
        .reduce((acc, r) => acc + r.latency, 0) / successCount || 0;
    const totalToolCalls = modelResults.reduce(
      (acc, r) => acc + r.toolCallCount,
      0
    );
    const avgOutputLength =
      modelResults
        .filter(r => r.success)
        .reduce((acc, r) => acc + r.outputLength, 0) / successCount || 0;

    console.log(`ã€${name}ã€‘`);
    console.log(
      `   æˆåŠŸç‡: ${successCount}/${modelResults.length} (${((successCount / modelResults.length) * 100).toFixed(0)}%)`
    );
    console.log(`   å¹³å‡å»¶è¿Ÿ: ${avgLatency.toFixed(0)}ms`);
    console.log(`   æ€»å·¥å…·è°ƒç”¨: ${totalToolCalls}æ¬¡`);
    console.log(`   å¹³å‡è¾“å‡ºé•¿åº¦: ${avgOutputLength.toFixed(0)}å­—ç¬¦`);
    console.log("");
  };

  summarize(grokResults, "Grok");
  summarize(glmResults, "GLM");

  // å¯¹æ¯”è¡¨æ ¼
  console.log("ğŸ“‹ è¯¦ç»†å¯¹æ¯”è¡¨\n");
  console.log(
    "| æµ‹è¯•ç”¨ä¾‹ | Grok æˆåŠŸ | Grok å»¶è¿Ÿ | Grok å·¥å…·æ•° | GLM æˆåŠŸ | GLM å»¶è¿Ÿ | GLM å·¥å…·æ•° |"
  );
  console.log(
    "|---------|----------|----------|------------|---------|---------|-----------|"
  );

  for (const testCase of TEST_CASES) {
    const grok = results.find(
      r => r.model === "Grok" && r.testCase === testCase.name
    )!;
    const glm = results.find(
      r => r.model === "GLM" && r.testCase === testCase.name
    )!;

    console.log(
      `| ${testCase.name.padEnd(8)} | ${grok.success ? "âœ…" : "âŒ"}       | ${String(grok.latency).padStart(7)}ms | ${String(grok.toolCallCount).padStart(10)} | ${glm.success ? "âœ…" : "âŒ"}      | ${String(glm.latency).padStart(6)}ms | ${String(glm.toolCallCount).padStart(9)} |`
    );
  }

  // ç»“è®º
  console.log("\n" + "=".repeat(70));
  console.log("ğŸ¯ ç»“è®º\n");

  const grokSuccessRate =
    grokResults.filter(r => r.success).length / grokResults.length;
  const glmSuccessRate =
    glmResults.filter(r => r.success).length / glmResults.length;
  const grokAvgLatency =
    grokResults.filter(r => r.success).reduce((acc, r) => acc + r.latency, 0) /
      grokResults.filter(r => r.success).length || Infinity;
  const glmAvgLatency =
    glmResults.filter(r => r.success).reduce((acc, r) => acc + r.latency, 0) /
      glmResults.filter(r => r.success).length || Infinity;
  const grokTotalTools = grokResults.reduce(
    (acc, r) => acc + r.toolCallCount,
    0
  );
  const glmTotalTools = glmResults.reduce((acc, r) => acc + r.toolCallCount, 0);

  console.log(
    `Tool Call ç¨³å®šæ€§: ${grokSuccessRate >= glmSuccessRate ? "ğŸŸ¢ Grok" : "ğŸ”µ GLM"} æ›´ç¨³å®š (${Math.max(grokSuccessRate, glmSuccessRate) * 100}%)`
  );
  console.log(
    `å“åº”é€Ÿåº¦: ${grokAvgLatency <= glmAvgLatency ? "ğŸŸ¢ Grok" : "ğŸ”µ GLM"} æ›´å¿« (${Math.min(grokAvgLatency, glmAvgLatency).toFixed(0)}ms)`
  );
  console.log(
    `å·¥å…·è°ƒç”¨ç§¯ææ€§: ${grokTotalTools >= glmTotalTools ? "ğŸŸ¢ Grok" : "ğŸ”µ GLM"} æ›´ç§¯æ (${Math.max(grokTotalTools, glmTotalTools)}æ¬¡)`
  );
}

// ==================== ç¨³å®šæ€§æµ‹è¯•ï¼ˆåŒä¸€é—®é¢˜å¤šæ¬¡è¿è¡Œï¼‰====================

async function runStabilityTest() {
  console.log("\n\nğŸ”„ ç¨³å®šæ€§æµ‹è¯•ï¼ˆåŒä¸€é—®é¢˜è¿è¡Œ3æ¬¡ï¼‰\n");
  console.log("=".repeat(70));

  const testPrompt = "åˆ†ææ¯”äºšè¿ªçš„æŠ€æœ¯é¢ï¼Œç»™å‡ºä¹°å–å»ºè®®";
  const runs = 3;

  for (const modelType of ["grok", "glm"] as const) {
    console.log(`\nã€${MODELS[modelType].name}ã€‘è¿è¡Œ ${runs} æ¬¡:\n`);

    const results: TestResult[] = [];

    for (let i = 0; i < runs; i++) {
      console.log(`   ç¬¬ ${i + 1} æ¬¡...`);
      const result = await runSingleTest(modelType, {
        name: `ç¨³å®šæ€§æµ‹è¯•-${i + 1}`,
        prompt: testPrompt,
        expectedTools: ["analyze_stock_technical"],
      });
      results.push(result);

      console.log(
        `      ${result.success ? "âœ…" : "âŒ"} | ${result.latency}ms | å·¥å…·: ${result.toolsUsed.join(", ") || "æ— "}`
      );
    }

    const successCount = results.filter(r => r.success).length;
    const latencies = results.filter(r => r.success).map(r => r.latency);
    const avgLatency =
      latencies.reduce((a, b) => a + b, 0) / latencies.length || 0;
    const latencyVariance =
      latencies.length > 1
        ? Math.sqrt(
            latencies.reduce((acc, l) => acc + Math.pow(l - avgLatency, 2), 0) /
              latencies.length
          )
        : 0;

    console.log(`\n   ğŸ“Š ç»Ÿè®¡:`);
    console.log(`      æˆåŠŸç‡: ${successCount}/${runs}`);
    console.log(`      å¹³å‡å»¶è¿Ÿ: ${avgLatency.toFixed(0)}ms`);
    console.log(`      å»¶è¿Ÿæ ‡å‡†å·®: ${latencyVariance.toFixed(0)}ms`);
  }
}

// ==================== ä¸»å‡½æ•° ====================

async function main() {
  try {
    await runComparisonTest();
    await runStabilityTest();

    console.log("\nâœ… æµ‹è¯•å®Œæˆ");
  } catch (error: any) {
    console.error("\nâŒ æµ‹è¯•å¤±è´¥:", error.message);
    console.error(error.stack);
  }
}

main();
