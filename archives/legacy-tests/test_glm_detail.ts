/**
 * GLM æ¨¡å‹è¯¦ç»†æµ‹è¯•è„šæœ¬
 * æµ‹è¯• glm-4.7 å’Œ glm-4.6 çš„å®Œæ•´å›å¤èƒ½åŠ›
 */

import "dotenv/config";

const GLM_API_URL =
  process.env.GLM_API_URL || "https://open.bigmodel.cn/api/paas/v4";
const GLM_API_KEY = process.env.GLM_API_KEY || "";

// è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
const MODELS_TO_TEST = ["glm-4.7", "glm-4.6", "glm-4-flash"];

// æ›´å¤æ‚çš„æµ‹è¯•é—®é¢˜
const TEST_PROMPT = `è¯·åˆ†æä¸€ä¸‹Aè‚¡å¸‚åœºä¸­ï¼Œæ•£æˆ·æŠ•èµ„è€…å¸¸è§çš„ä¸‰ä¸ªå¿ƒç†è¯¯åŒºï¼Œå¹¶ç»™å‡ºç›¸åº”çš„æ”¹è¿›å»ºè®®ã€‚
è¦æ±‚ï¼š
1. æ¯ä¸ªè¯¯åŒºè¦æœ‰å…·ä½“çš„ä¾‹å­
2. å»ºè®®è¦æœ‰å¯æ“ä½œæ€§
3. å›ç­”è¦ç»“æ„æ¸…æ™°`;

async function testModel(model: string): Promise<void> {
  console.log(`\n${"=".repeat(60)}`);
  console.log(`ğŸ§ª æµ‹è¯•æ¨¡å‹: ${model}`);
  console.log("=".repeat(60));

  const startTime = Date.now();

  try {
    const response = await fetch(`${GLM_API_URL}/chat/completions`, {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${GLM_API_KEY}`,
      },
      body: JSON.stringify({
        model: model,
        messages: [
          {
            role: "system",
            content: "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„Aè‚¡æŠ•èµ„é¡¾é—®ï¼Œæœ‰ä¸°å¯Œçš„å¸‚åœºåˆ†æç»éªŒã€‚",
          },
          {
            role: "user",
            content: TEST_PROMPT,
          },
        ],
        max_tokens: 1000,
        temperature: 0.7,
      }),
    });

    const endTime = Date.now();
    const duration = ((endTime - startTime) / 1000).toFixed(2);

    if (!response.ok) {
      const errorText = await response.text();
      console.log(`âŒ å¤±è´¥ (${response.status})`);
      console.log(`é”™è¯¯ä¿¡æ¯: ${errorText}`);
      return;
    }

    const data = await response.json();
    const content = data.choices?.[0]?.message?.content || "æ— å“åº”";
    const usage = data.usage;
    const finishReason = data.choices?.[0]?.finish_reason;

    console.log(`\nâœ… æˆåŠŸ! (è€—æ—¶: ${duration}s)`);
    console.log(
      `ğŸ“Š Tokenä½¿ç”¨: prompt=${usage?.prompt_tokens}, completion=${usage?.completion_tokens}, total=${usage?.total_tokens}`
    );
    console.log(`ğŸ ç»“æŸåŸå› : ${finishReason}`);
    console.log(`\nğŸ“ å®Œæ•´å›å¤:\n${"-".repeat(60)}`);
    console.log(content);
    console.log("-".repeat(60));
  } catch (error: any) {
    console.log(`âŒ è¯·æ±‚é”™è¯¯: ${error.message}`);
  }
}

async function main() {
  console.log("\n" + "ğŸ”¬".repeat(30));
  console.log("æ™ºè°±AI GLM æ¨¡å‹è¯¦ç»†æµ‹è¯•");
  console.log("ğŸ”¬".repeat(30));
  console.log(`\nAPI URL: ${GLM_API_URL}`);
  console.log(`API Key: ${GLM_API_KEY.substring(0, 15)}...`);
  console.log(`\nğŸ“‹ æµ‹è¯•é—®é¢˜:\n${TEST_PROMPT}`);

  for (const model of MODELS_TO_TEST) {
    await testModel(model);
  }

  console.log("\n" + "âœ¨".repeat(30));
  console.log("æ‰€æœ‰æµ‹è¯•å®Œæˆ!");
  console.log("âœ¨".repeat(30) + "\n");
}

main();
